{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a977a6-096e-4ee1-887f-7717e4603b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7edc68af-31ab-498b-b962-8a717e42af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b44c725f-6f77-4104-ad39-44432dceaddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9f199-82da-4a67-a8b4-003cf5781867",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52c68296-7d30-413a-a176-adf2af37e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsela_path = 'newsela/'\n",
    "newsela_metadata = os.path.join(newsela_path, 'articles_metadata.csv')\n",
    "newsela_articles_dir = os.path.join(newsela_path, 'articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95c1fb39-4783-4124-8be3-eca6003593f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slug</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>grade_level</th>\n",
       "      <th>version</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10dollarbill-woman</td>\n",
       "      <td>en</td>\n",
       "      <td>Tubman, Perkins or Roosevelt? Woman on $10 bil...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10dollarbill-woman.en.0.txt</td>\n",
       "      <td>WASHINGTON â€” An abolitionist. The longest-serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10dollarbill-woman</td>\n",
       "      <td>en</td>\n",
       "      <td>Americans weigh in to choose the woman who wil...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10dollarbill-woman.en.1.txt</td>\n",
       "      <td>WASHINGTON â€” The all-male lineup on American m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10dollarbill-woman</td>\n",
       "      <td>en</td>\n",
       "      <td>The $10 question: Who will be the new face on ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10dollarbill-woman.en.2.txt</td>\n",
       "      <td>WASHINGTON â€” It's time for a woman to be honor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10dollarbill-woman</td>\n",
       "      <td>en</td>\n",
       "      <td>New $10 bill will have a theme and a woman's p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10dollarbill-woman.en.3.txt</td>\n",
       "      <td>WASHINGTON â€” It is time that a woman be on Ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10dollarbill-woman</td>\n",
       "      <td>en</td>\n",
       "      <td>We will soon have an American woman's face on ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>10dollarbill-woman.en.4.txt</td>\n",
       "      <td>WASHINGTON â€” Pictures of men are on all Americ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 slug language  \\\n",
       "0  10dollarbill-woman       en   \n",
       "1  10dollarbill-woman       en   \n",
       "2  10dollarbill-woman       en   \n",
       "3  10dollarbill-woman       en   \n",
       "4  10dollarbill-woman       en   \n",
       "\n",
       "                                               title  grade_level  version  \\\n",
       "0  Tubman, Perkins or Roosevelt? Woman on $10 bil...         12.0        0   \n",
       "1  Americans weigh in to choose the woman who wil...          8.0        1   \n",
       "2  The $10 question: Who will be the new face on ...          6.0        2   \n",
       "3  New $10 bill will have a theme and a woman's p...          5.0        3   \n",
       "4  We will soon have an American woman's face on ...          3.0        4   \n",
       "\n",
       "                      filename  \\\n",
       "0  10dollarbill-woman.en.0.txt   \n",
       "1  10dollarbill-woman.en.1.txt   \n",
       "2  10dollarbill-woman.en.2.txt   \n",
       "3  10dollarbill-woman.en.3.txt   \n",
       "4  10dollarbill-woman.en.4.txt   \n",
       "\n",
       "                                                text  \n",
       "0  WASHINGTON â€” An abolitionist. The longest-serv...  \n",
       "1  WASHINGTON â€” The all-male lineup on American m...  \n",
       "2  WASHINGTON â€” It's time for a woman to be honor...  \n",
       "3  WASHINGTON â€” It is time that a woman be on Ame...  \n",
       "4  WASHINGTON â€” Pictures of men are on all Americ...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(newsela_metadata)\n",
    "df = pd.read_csv('newsela_all.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579b6c2-966d-428e-95f5-5208cffa4f71",
   "metadata": {},
   "source": [
    "### One-time Processing for text ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "259831a6-3c2c-44b2-bab7-241279939a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             return file.read()\n",
    "#     except Exception as e:\n",
    "#         # In case of error, return NaN or some error indication\n",
    "#         return pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e50e745-b684-45d7-b321-34aca5f97117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = []\n",
    "\n",
    "# for i, row in enumerate(df.itertuples()):\n",
    "#     file_path = os.path.join(newsela_articles_dir, row.filename)\n",
    "#     # print(file_path)\n",
    "#     file_text = read_file(file_path)\n",
    "#     # print(file_text)\n",
    "#     texts.append(file_text)\n",
    "\n",
    "# df['text'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "625fac25-e880-4c39-b98e-ccd854f86df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a488e9e-55a8-4d9e-a8fd-cabff4291bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('newsela_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2355b9-1a15-4c00-9bbf-8d1cb6630fb8",
   "metadata": {},
   "source": [
    "### Create Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1d3961b-32c3-4371-a96b-fa3d743de6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 0 - 8629; Test index: 8630 - 10786\n"
     ]
    }
   ],
   "source": [
    "train_index = 8629 # 80% of the entire set\n",
    "test_index = df.shape[0] - train_index\n",
    "print(f\"Train rows: 0 - {train_index}; Test index: {train_index + 1} - {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "995b0920-172e-414f-96cb-ad32ca75aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8629, 7) (2157, 7)\n"
     ]
    }
   ],
   "source": [
    "train_df = df[:train_index]\n",
    "test_df = df[train_index:]\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bdcb531-b1f6-404f-ac9a-191a09ec305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\"train\": [], \"test\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06e66e-c489-426c-8868-b7d4e1c780a8",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45f5663d-5df8-4ba1-887e-4b3d0af4d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = train_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "344a353f-9400-4776-aece-09af997ec502",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_slug = train_dict[0]['slug']\n",
    "tmp_texts = []\n",
    "tmp_titles = []\n",
    "\n",
    "for record in train_dict:\n",
    "    # Iterate through all the records belonging to a slug and save the texts\n",
    "    if record['slug'] == prev_slug:\n",
    "        tmp_texts.append(record['text'])\n",
    "        tmp_titles.append(record['title'])\n",
    "    # We've reached a new slug, so it's time to create the train/test record for the given slug\n",
    "    else:\n",
    "        for title, text in zip(tmp_titles[:-1], tmp_texts[:-1]):\n",
    "            dataset_dict[\"train\"].append(\n",
    "                {\n",
    "                    'text': text,\n",
    "                    'simplification': tmp_texts[-1],\n",
    "                    'title': title\n",
    "                }\n",
    "            )\n",
    "        prev_slug = record['slug']\n",
    "        prev_title = record['title']\n",
    "        tmp_texts = []       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c4fbe04-a9ad-429a-b045-d7de2ce3d0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5182\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_dict['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5a5fc-8d78-45be-9705-10ac1e48ea59",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6742fdf2-e778-4201-910e-13b0df15b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = test_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ac5fe2f-16f4-4e91-a23d-9ccf4f3d4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_slug = train_dict[0]['slug']\n",
    "tmp_texts = []\n",
    "tmp_titles = []\n",
    "\n",
    "for record in test_dict:\n",
    "    # Iterate through all the records belonging to a slug and save the texts\n",
    "    if record['slug'] == prev_slug:\n",
    "        tmp_texts.append(record['text'])\n",
    "        tmp_titles.append(record['title'])\n",
    "    # We've reached a new slug, so it's time to create the train/test record for the given slug\n",
    "    else:\n",
    "        for title, text in zip(tmp_titles[:-1], tmp_texts[:-1]):\n",
    "            dataset_dict[\"test\"].append(\n",
    "                {\n",
    "                    'text': text,\n",
    "                    'simplification': tmp_texts[-1],\n",
    "                    'title': title\n",
    "                }\n",
    "            )\n",
    "        prev_slug = record['slug']\n",
    "        prev_title = record['title']\n",
    "        tmp_texts = []       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bef6a1d3-557e-43a4-83ce-96f41eb21b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_dict['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9288874c-846d-40a3-9fcf-c1a78f945e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"simplify: \"\n",
    "inputs = []\n",
    "targets = []\n",
    "for doc in dataset_dict['train']:\n",
    "    inputs.append(prefix + doc['text'])\n",
    "    targets.append(doc['simplification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca5c1033-332b-4181-8757-98c34f8ff740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5182 5182\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs), len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ca39f34-4240-4225-bbcf-40cb3d8dd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"simplify: \"\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "for doc in dataset_dict['test']:\n",
    "    test_inputs.append(prefix + doc['text'])\n",
    "    test_targets.append(doc['simplification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f723c02-080a-4089-a366-89ce1e26bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292 1292\n"
     ]
    }
   ],
   "source": [
    "print(len(test_inputs), len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced1d39-db6b-4181-8a98-465ed4406fb1",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11714805-d1be-4810-b368-2ab2b86c488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e12c2c7173444dbeb011a6c9d78941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54beecb84d94694ba88ddabcc0dd8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae6481889044e9383c97ebf0d1d2464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d7f9efadc04fc3aea907ac1ac4d187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "# tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4fb6f-6c04-413c-88dd-7ef4c28121de",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4b1cd6a-41b6-4300-91c4-71a99b2e4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(inputs, targets):\n",
    "    # inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, return_tensors='pt', max_length=1024, padding='max_length', truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=targets, return_tensors='pt', max_length=1024, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87c2fd97-649f-45ac-9905-73ed7bf64c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs, model_labels = preprocess_function(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f4dda3-3803-4433-9676-aafe0ac989f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20375,    60,  9874,  ...,   461,     4,     3]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c37e190f-be5d-434b-a2f4-cea0b5fbf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids, pad_token_id):\n",
    "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
    "    prev_output_tokens = input_ids.clone()\n",
    "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
    "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
    "    return prev_output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5860d-26ac-4fe3-af46-009a418fd8ac",
   "metadata": {},
   "source": [
    "### Train DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "984a1e4b-c98e-46af-8224-1b6ba4d75108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(model_inputs['input_ids'], model_labels['input_ids'])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d5952-dac8-4b92-b404-6b4948fdf008",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68efa4c0-a10d-4efc-9992-814cdebb377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "model.load_state_dict(torch.load(f'xlmnet_bs4_max_length1024_4e.pth')['model_state_dict'])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e7db043-4445-4345-9785-2ee7eb027791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Sergiu/Desktop/DLComp/venv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 4. Define Training Parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb228fc8-26cf-4c55-9ebb-5fccd6660f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7e73698-6279-4c01-9d13-5985d7918cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step 100 Loss 2.9248955249786377\n",
      "Epoch 0 Step 200 Loss 3.905564069747925\n",
      "Epoch 0 Step 300 Loss 4.352330207824707\n",
      "Epoch 0 Step 400 Loss 3.610839366912842\n",
      "Epoch 0 Step 500 Loss 3.7187016010284424\n",
      "Epoch 0 Step 600 Loss 3.744738817214966\n",
      "Epoch 0 Step 700 Loss 4.7788238525390625\n",
      "Epoch 0 Step 800 Loss 3.6326682567596436\n",
      "Epoch 0 Step 900 Loss 4.585256576538086\n",
      "Epoch 0 Step 1000 Loss 5.124723434448242\n",
      "Epoch 0 Step 1100 Loss 3.506408452987671\n",
      "Epoch 0 Step 1200 Loss 3.986222743988037\n",
      "Average Training Loss: 4.112069247129523\n",
      "Epoch 1 Step 100 Loss 5.028563022613525\n",
      "Epoch 1 Step 200 Loss 4.4681596755981445\n",
      "Epoch 1 Step 300 Loss 4.1537580490112305\n",
      "Epoch 1 Step 400 Loss 4.318265438079834\n",
      "Epoch 1 Step 500 Loss 4.0916666984558105\n",
      "Epoch 1 Step 600 Loss 4.466844081878662\n",
      "Epoch 1 Step 700 Loss 3.6207094192504883\n",
      "Epoch 1 Step 800 Loss 4.858518123626709\n",
      "Epoch 1 Step 900 Loss 4.145331859588623\n",
      "Epoch 1 Step 1000 Loss 4.293227195739746\n",
      "Epoch 1 Step 1100 Loss 3.7182812690734863\n",
      "Epoch 1 Step 1200 Loss 3.953618288040161\n",
      "Average Training Loss: 4.051214164237917\n",
      "Epoch 2 Step 100 Loss 4.757606029510498\n",
      "Epoch 2 Step 200 Loss 5.168574810028076\n",
      "Epoch 2 Step 300 Loss 4.199620723724365\n",
      "Epoch 2 Step 400 Loss 4.727364540100098\n",
      "Epoch 2 Step 500 Loss 3.182166337966919\n",
      "Epoch 2 Step 600 Loss 3.402705669403076\n",
      "Epoch 2 Step 700 Loss 3.4500253200531006\n",
      "Epoch 2 Step 800 Loss 3.785095691680908\n",
      "Epoch 2 Step 900 Loss 4.2942705154418945\n",
      "Epoch 2 Step 1000 Loss 2.666578531265259\n",
      "Epoch 2 Step 1100 Loss 4.8203887939453125\n",
      "Epoch 2 Step 1200 Loss 4.158896446228027\n",
      "Average Training Loss: 4.0015165649446445\n",
      "Epoch 3 Step 100 Loss 3.861858367919922\n",
      "Epoch 3 Step 200 Loss 3.83013916015625\n",
      "Epoch 3 Step 300 Loss 3.580054521560669\n",
      "Epoch 3 Step 400 Loss 3.796778917312622\n",
      "Epoch 3 Step 500 Loss 3.753565549850464\n",
      "Epoch 3 Step 600 Loss 3.8697052001953125\n",
      "Epoch 3 Step 700 Loss 4.7443695068359375\n",
      "Epoch 3 Step 800 Loss 4.254483699798584\n",
      "Epoch 3 Step 900 Loss 3.378174066543579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_loss = 3\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Assuming batch is a tuple of (input_ids, target_ids)\n",
    "        input_ids, target_ids = batch\n",
    "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "        \n",
    "        # Shift the target IDs for the decoder input and ignore the last token\n",
    "        decoder_input_ids = shift_tokens_right(target_ids, tokenizer.pad_token_id)\n",
    "\n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=decoder_input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if total_loss < min_loss:\n",
    "            min_loss = total_loss\n",
    "\n",
    "            checkpoint = {\n",
    "               'model_state_dict': model.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, f'xlmnet_bs4_max_length_1024_epoch_{epoch}.pth')\n",
    "        \n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print(f'Epoch {epoch} Step {step} Loss {loss.item()}')\n",
    "\n",
    "          \n",
    "\n",
    "    # Calculate the average loss over the entire batch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average Training Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd8fe5-b679-47d3-b511-9c5a85524947",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 0 Step 100 Loss 5.026244163513184\n",
    "Epoch 0 Step 200 Loss 5.893657684326172\n",
    "Epoch 0 Step 300 Loss 4.964169979095459\n",
    "Epoch 0 Step 400 Loss 5.1959733963012695\n",
    "Epoch 0 Step 500 Loss 4.505870342254639\n",
    "Epoch 0 Step 600 Loss 4.594396591186523\n",
    "Epoch 0 Step 700 Loss 3.460613965988159\n",
    "Epoch 0 Step 800 Loss 4.547153472900391\n",
    "Epoch 0 Step 900 Loss 4.283635139465332\n",
    "Epoch 0 Step 1000 Loss 3.9535672664642334\n",
    "Epoch 0 Step 1100 Loss 4.520374298095703\n",
    "Epoch 0 Step 1200 Loss 6.07244348526001\n",
    "Average Training Loss: 4.940993779235416\n",
    "Epoch 1 Step 100 Loss 5.125080585479736\n",
    "Epoch 1 Step 200 Loss 3.9568779468536377\n",
    "Epoch 1 Step 300 Loss 4.50228214263916\n",
    "Epoch 1 Step 400 Loss 5.492114543914795\n",
    "Epoch 1 Step 500 Loss 4.65526008605957\n",
    "Epoch 1 Step 600 Loss 5.31953763961792\n",
    "Epoch 1 Step 700 Loss 3.8569183349609375\n",
    "Epoch 1 Step 800 Loss 4.194915294647217\n",
    "Epoch 1 Step 900 Loss 4.093446254730225\n",
    "Epoch 1 Step 1000 Loss 3.662853479385376\n",
    "Epoch 1 Step 1100 Loss 4.537163734436035\n",
    "Epoch 1 Step 1200 Loss 4.524852752685547\n",
    "Average Training Loss: 4.4248728595766025\n",
    "Epoch 2 Step 100 Loss 4.603614330291748\n",
    "Epoch 2 Step 200 Loss 4.217392921447754\n",
    "Epoch 2 Step 300 Loss 4.435378074645996\n",
    "Epoch 2 Step 400 Loss 4.7331438064575195\n",
    "Epoch 2 Step 500 Loss 4.359727382659912\n",
    "Epoch 2 Step 600 Loss 5.301181793212891\n",
    "Epoch 2 Step 700 Loss 4.332385063171387\n",
    "Epoch 2 Step 800 Loss 3.7514963150024414\n",
    "Epoch 2 Step 900 Loss 4.650548934936523\n",
    "Epoch 2 Step 1000 Loss 4.121835231781006\n",
    "Epoch 2 Step 1100 Loss 3.826198101043701\n",
    "Epoch 2 Step 1200 Loss 5.222508907318115\n",
    "Average Training Loss: 4.263375194536315\n",
    "Epoch 3 Step 100 Loss 4.717140197753906\n",
    "Epoch 3 Step 200 Loss 4.590120315551758\n",
    "Epoch 3 Step 300 Loss 4.780978202819824\n",
    "Epoch 3 Step 400 Loss 4.299923896789551\n",
    "Epoch 3 Step 500 Loss 4.794330596923828\n",
    "Epoch 3 Step 600 Loss 4.90772008895874\n",
    "Epoch 3 Step 700 Loss 5.2156596183776855\n",
    "Epoch 3 Step 800 Loss 4.793715953826904\n",
    "Epoch 3 Step 900 Loss 3.985318660736084\n",
    "Epoch 3 Step 1000 Loss 3.7246477603912354\n",
    "Epoch 3 Step 1100 Loss 3.602402687072754\n",
    "Epoch 3 Step 1200 Loss 4.329013347625732\n",
    "Average Training Loss: 4.173481227071197\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd386cc4-eea9-4af7-a691-b9fbe36981a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "   'model_state_dict': model.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, f'xlmnet_bs4_max_length1024_4e.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4c25b-c947-463c-bd71-5f43eb0e4ffd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc5fc5d8-fa82-49a8-aa29-15183624455c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetLMHeadModel(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_loss): Linear(in_features=768, out_features=32000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')\n",
    "model.load_state_dict(torch.load(f'xlmnet_bs4_max_length1024_4e.pth')['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b7fb265-1980-41fb-9d44-f83c5dc1e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test_inputs, model_test_labels = preprocess_function(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98557adb-441e-44a2-bda6-54a0884baa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset\n",
    "test_dataset = TensorDataset(model_test_inputs['input_ids'], model_test_labels['input_ids'])\n",
    "\n",
    "# Create a DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07dc007-0395-48fa-a91b-7bab9be00299",
   "metadata": {},
   "source": [
    "### Generate Rouge Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93179b12-6c19-4156-9ab9-9e0a5b49edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def evaluate_rouge(model, val_loader, tokenizer, device):\n",
    "    rouge = load_metric('rouge', trust_remote_code=True)\n",
    "    model.eval()\n",
    "    for batch in val_loader:\n",
    "        input_ids, target_ids = batch\n",
    "        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "        \n",
    "        # Generate summaries\n",
    "        outputs = model.generate(input_ids=input_ids)\n",
    "        \n",
    "        # Decode the summaries\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Compute Rouge score\n",
    "        rouge.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # Calculate final score\n",
    "    result = rouge.compute()\n",
    "    for key in result.keys():\n",
    "        print(f\"{key}: {result[key].mid}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820cb1e-c52f-4317-a8ba-ba54f147516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1859/3823000111.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric('rouge', trust_remote_code=True)\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (-1). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rouge_score = evaluate_rouge(model, test_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3a9f9-354a-4228-b93a-c1b245a30020",
   "metadata": {},
   "source": [
    "ROUGE Score computed on last 20% of the NEWSELA dataset.\n",
    "```py\n",
    "rouge1: Score(precision=0.5682628637119129, recall=0.7606626781061232, fmeasure=0.6417841616651105)\r\n",
    "rouge2: Score(precision=0.3472802895392726, recall=0.4498331452513754, fmeasure=0.3873985543099933)\r\n",
    "rougeL: Score(precision=0.3939804996636087, recall=0.5136579508637882, fmeasure=0.4403192447961841)\r\n",
    "rougeLsum: Score(precision=0.3938001014396969, recall=0.5137175834759207, fmeasure=0.440159675368905\n",
    "```16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9ea2b-9ca0-460c-8062-39089ee7ff81",
   "metadata": {},
   "source": [
    "### Generate Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "043bb9df-d7c5-4736-a5a0-248a91b18a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simplification(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=1024, padding='max_length', truncation=True).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    simplification_ids = model.generate(inputs['input_ids'], min_length=40, max_length=1025, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    simplification = tokenizer.decode(simplification_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86908831-6539-46e2-ba86-7843a4335ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Coronaviruses are a group of related RNA viruses that cause diseases in mammals and birds. In humans and birds, they cause respiratory tract infections that can range from mild to lethal. Mild illnesses in humans include some cases of the common cold (which is also caused by other viruses, predominantly rhinoviruses), while more lethal varieties can cause SARS, MERS and COVID-19, which is causing the ongoing pandemic. In cows and pigs they cause diarrhea, while in mice they cause hepatitis and encephalomyelitis.\n",
    "\n",
    "Coronaviruses constitute the subfamily Orthocoronavirinae, in the family Coronaviridae, order Nidovirales and realm Riboviria.[3][4] They are enveloped viruses with a positive-sense single-stranded RNA genome and a nucleocapsid of helical symmetry.[5] The genome size of coronaviruses ranges from approximately 26 to 32 kilobases, one of the largest among RNA viruses.[6] They have characteristic club-shaped spikes that project from their surface, which in electron micrographs create an image reminiscent of the stellar corona, from which their name derives.[7]\n",
    "\n",
    "Etymology\n",
    "The name \"coronavirus\" is derived from Latin corona, meaning \"crown\" or \"wreath\", itself a borrowing from Greek ÎºÎ¿ÏÏŽÎ½Î· korá¹“nÄ“, \"garland, wreath\".[8][9] The name was coined by June Almeida and David Tyrrell who first observed and studied human coronaviruses.[10] The word was first used in print in 1968 by an informal group of virologists in the journal Nature to designate the new family of viruses.[7] The name refers to the characteristic appearance of virions (the infective form of the virus) by electron microscopy, which have a fringe of large, bulbous surface projections creating an image reminiscent of the solar corona or halo.[7][10] This morphology is created by the viral spike peplomers, which are proteins on the surface of the virus.[11]\n",
    "\n",
    "The scientific name Coronavirus was accepted as a genus name by the International Committee for the Nomenclature of Viruses (later renamed International Committee on Taxonomy of Viruses) in 1971.[12] As the number of new species increased, the genus was split into four genera, namely Alphacoronavirus, Betacoronavirus, Deltacoronavirus, and Gammacoronavirus in 2009.[13] The common name coronavirus is used to refer to any member of the subfamily Orthocoronavirinae.[4] As of 2020, 45 species are officially recognised.[14]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8150c6be-1f26-4eb5-9fec-daf9b38dbf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplification = generate_simplification(prompt, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbcfc6b2-ee5c-49ad-a645-8283d3b0629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronaviruses are a group of related RNA viruses that cause diseases in mammals and birds. In humans and birds, they cause respiratory tract infections that can range from mild to lethal. Mild illnesses in humans include some cases of the common cold (which is also caused by other viruses, predominantly rhinoviruses), while more lethal varieties can cause SARS, MERS and COVID-19, which is causing the ongoing pandemic. In cows and pigs they cause diarrhea, while in mice they cause hepatitis and encephalomyelitis. Coronaviruses constitute the subfamily Orthocoronavirinae, in the family Coronaviridae, order Nidovirales and realm Riboviria.[3][4] They are enveloped viruses with a positive-sense single-stranded RNA genome and a nucleocapsid of helical symmetry.[5] The genome size of coronaviruses ranges from approximately 26 to 32 kilobases, one of the largest among RNA viruses.[6] They have characteristic club-shaped spikes that project from their surface, which in electron micrographs create an image reminiscent of the stellar corona, from which their name derives.[7] Etymology The name \"coronavirus\" is derived from Latin corona, meaning \"crown\" or \"wreath\", itself a borrowing from Greek Î¿ korone, \"garland, wreath\".[8][9] The name was coined by June Almeida and David Tyrrell who first observed and studied human coronaviruses.[10] The word was first used in print in 1968 by an informal group of virologists in the journal Nature to designate the new family of viruses.[7] The name refers to the characteristic appearance of virions (the infective form of the virus) by electron microscopy, which have a fringe of large, bulbous surface projections creating an image reminiscent of the solar corona or halo.[7][10] This morphology is created by the viral spike peplomers, which are proteins on the surface of the virus.[11] The scientific name Coronavirus was accepted as a genus name by the International Committee for the Nomenclature of Viruses (later renamed International Committee on Taxonomy of Viruses) in 1971.[12] As the number of new species increased, the genus was split into four genera, namely Alphacoronavirus, Betacoronavirus, Deltacoronavirus, and Gammacoronavirus in 2009.[13] The common name coronavirus is used to refer to any member of the subfamily Orthocoronavirinae.[4] As of 2020, 45 species are officially recognised.[14].\n"
     ]
    }
   ],
   "source": [
    "print(simplification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1724a-75f0-4c05-aef5-fcc3e72f8ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
