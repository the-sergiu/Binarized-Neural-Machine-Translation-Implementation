{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656b2f34-0bbb-46a5-90ae-f11e95eab944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Iterable, List\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3306a-b975-4766-b023-1b256abaaa0d",
   "metadata": {},
   "source": [
    "## Dataset and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fcf9747-8cd3-4ab8-9091-6e76cb4a38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generator function to yield a list of tokens for each sample in the dataset.\n",
    "    \"\"\"\n",
    "    # Mapping of languages to their respective indices in the dataset\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    # Iterate over each data sample in the dataset\n",
    "    for data_sample in data_iter:\n",
    "        # Tokenize the sample using the specified language's token transformation function\n",
    "        # and yield the list of tokens\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048834f3-7588-4ee8-8cb5-53097be286b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'de'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38d7813-3ce5-40d4-8fed-11812d948ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d24bbb-f661-4af6-ba65-d8ece15d2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    \"\"\"\n",
    "    Compose several transforms sequentially.\n",
    "\n",
    "    This function is a utility to apply a list of transformations sequentially to text data.\n",
    "    It is used for data preprocessing steps.\n",
    "    \"\"\"\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    \"\"\"\n",
    "    Add BOS (beginning of sequence) and EOS (end of sequence) tokens and convert to tensor.\n",
    "\n",
    "    This function is used to preprocess the token sequence for the transformer model by adding\n",
    "    special tokens and converting the list of token IDs into a tensor.\n",
    "    \"\"\"\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fc7bf8-4d3f-4114-b69c-f511b890d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function to collate data samples into batch tensors.\n",
    "\n",
    "    This function is used during data loading to collate multiple data samples (source and target)\n",
    "    into batch tensors. It also applies padding to ensure consistent tensor sizes.\n",
    "\n",
    "    Applied within DataLoaders to ensure all samples are valid for the training process.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # Apply text transformations and add to batches\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    # Pad sequences to create uniformly sized batches\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c661c7-6bb4-46d5-b34e-f64249c14223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Generate a square mask for the sequence. The mask shows which entries should not be used.\n",
    "    \n",
    "    This mask is used in the decoder part of the transformer model to prevent the model from\n",
    "    peeking at the subsequent positions in the sequence during training.\n",
    "    \"\"\"\n",
    "    # Create an upper triangular matrix of ones\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    \n",
    "    # Replace 0s with '-inf' and 1s with 0.0. '-inf' values will be masked.\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    \"\"\"\n",
    "    Create masks for the source and target sequences.\n",
    "\n",
    "    This function generates masks for the source and target to be used in the transformer model.\n",
    "    These masks include padding masks for both source and target, and a target mask for the subsequent positions.\n",
    "    \"\"\"\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # Create a subsequent mask for target sequences to prevent future peeks\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    # Source mask is all zeros since full sequence can be attended to\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    # Padding masks for source and target sequences\n",
    "    # This marks positions with PAD_IDX as True so that these are not used in attention computations\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8bf2d0-1844-4251-8140-ac79330a13cc",
   "metadata": {},
   "source": [
    "## Model\n",
    "- We use a 6L6L Transformer as the base model.\n",
    "- Embedding dimension is 1024. Each multi-head attention layer has 16 heads, with a dimension of 1024 for \n",
    "QKV if combining all the heads.\n",
    "- The hidden projection dimension in FFNs is 4096.\n",
    "- Dropout layers has a dropout rate of 0.1.\n",
    "- We use a batch size of 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa41ee4-807b-4964-9915-2c4e4cf4820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionalEncoding module injects some information about the relative or absolute position \n",
    "    of the tokens in the sequence. The positional encodings have the same dimension as \n",
    "    the embeddings so that the two can be summed. Here, we use sine and cosine functions \n",
    "    of different frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create constant 'denominator' part of the positional encoding formula\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "\n",
    "        # Apply sine to even indices in the array; 2i\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "\n",
    "        # Apply cosine to odd indices in the array; 2i+1\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "\n",
    "        # Reshape for adding to token embeddings\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Register pos_embedding as a buffer\n",
    "        # A buffer is a persistent state for the module (not a parameter, so it's not updated during backprop)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # Add positional encoding to token embedding, and apply dropout\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e34a53-b68f-4e78-a2c6-b21a06cb2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    TokenEmbedding module converts input indices (token IDs) into corresponding embeddings.\n",
    "    It's a wrapper around the PyTorch Embedding layer, scaling the embeddings by the square root \n",
    "    of the embedding size to normalize their variance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "\n",
    "        # Create an embedding layer with given vocabulary size and embedding size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # Store the embedding size, used for scaling the embeddings\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        # Convert token indices to embeddings\n",
    "        # Scale embeddings by the square root of the embedding size\n",
    "        # This scaling helps maintain a balance in the magnitude of the embeddings, \n",
    "        # especially useful in models that involve a lot of matrix multiplications (like Transformers)\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58265cc8-2636-4926-ba07-de759c6b19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SixLTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    SixLTransformer is a custom implementation of the Transformer model.\n",
    "    It consists of a Transformer, embedding layers for source and target, \n",
    "    a positional encoding layer, and a final linear layer (generator) for output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 1024,\n",
    "                 dropout: float = 0.1):\n",
    "        super(SixLTransformer, self).__init__()\n",
    "\n",
    "        # Transformer model\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # Final linear layer to generate output\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "        # Embedding layers for source and target sequences\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "\n",
    "        # Positional Encoding layer\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "\n",
    "        # Apply embeddings and positional encoding to the source and target input\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        # Pass through the transformer model\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        # Generate final output\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Encode source sequence into context vectors.\n",
    "        \"\"\"\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Decode target sequence from the encoded context.\n",
    "        \"\"\"\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0aad92a-cd02-4de0-9d9a-d247d38094c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 1024\n",
    "NHEAD = 16\n",
    "FFN_HID_DIM = 4096\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "\n",
    "model = SixLTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397bb55-74a7-4fc3-bc06-c93ac9eae043",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "- Adam optimizer (Kingma & Ba, 2014) is used with β1 = 0.9 and β2 = 0.98.\n",
    "- No weight decay is applied.\n",
    "- Base learning rate is 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b6b01ae-8e14-4195-9241-195e45b70c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac5462-928d-4df9-9e47-9915cb50276d",
   "metadata": {},
   "source": [
    "## Scheduler\n",
    "- We adopt a three-stage training scheme, where the learning rate (LR) of each stage decreases from base to zero following a cosine decay.\n",
    "- The first LR cycle has 50000 steps, others have 88339 steps.\n",
    "- A quantization event starts at the beginning of each stage.\n",
    "- **We first train the model in float.**\n",
    "- In the second stage, all weights will be binarized.\n",
    "- In the last stage, both weights and activations will be binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ca6d86-a4b4-412a-8667-0234298c8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler First Cycle\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50000)  # Adjust T_max based on training stages\n",
    "\n",
    "# Scheduler Second and Third Cycle\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=88339)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e0e9c-9ba5-488f-b181-5124de5baa09",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1a4f6f-48c4-45c3-bb6e-d3110cb0ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the normal Cross Entropy Loss, used by default when training the Transformer Architecture\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c8e75-454f-4e8e-96ba-28004e6c9e2e",
   "metadata": {},
   "source": [
    "## Proof of Concept Knowledge Distillation Loss\n",
    "- The authors apply knowledge distillation (KD) during training, learning from a more advanced model.\n",
    "- KD can be implemented by replacing the ground truth label in the cross-entropy loss function with the softmaxed logits from the teacher model, so it is optional for users.\n",
    "\n",
    "Implementing Knowledge Distillation is quite a specific task - it generally requires having access to a good model trained on the task at hand, and which is also efficient enough to run during the training process, whilst providing a valuable loss to the model. The computation requirements can get out of hand given given that 2 models should run at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a34aa3e-e9db-4200-ac86-b8ede22e24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target, teacher_model=None, alpha=0.5, temperature=2.0):\n",
    "    \"\"\"\n",
    "    Custom loss function for knowledge distillation.\n",
    "    \n",
    "    Parameters:\n",
    "    output: Logits from the student model.\n",
    "    target: Ground truth labels or logits from the teacher model.\n",
    "    teacher_model: Pre-trained teacher model. If None, standard training is performed.\n",
    "    alpha: Weighting factor for combining the teacher and student loss.\n",
    "    temperature: Temperature for softening probabilities.\n",
    "    \"\"\"\n",
    "    student_loss = F.cross_entropy(output, target.view(-1))\n",
    "    \n",
    "    if teacher_model:\n",
    "        with torch.no_grad():\n",
    "            teacher_output = teacher_model(target)\n",
    "        teacher_output = teacher_output.view_as(output)\n",
    "\n",
    "        # Soften the outputs of both student and teacher models\n",
    "        soft_student_probs = F.log_softmax(output / temperature, dim=1)\n",
    "        soft_teacher_probs = F.softmax(teacher_output / temperature, dim=1)\n",
    "\n",
    "        # Calculate the distillation loss\n",
    "        distillation_loss = F.kl_div(soft_student_probs, soft_teacher_probs, reduction='batchmean')\n",
    "\n",
    "        # Combine the student and distillation loss\n",
    "        return alpha * distillation_loss + (1 - alpha) * student_loss\n",
    "    else:\n",
    "        return student_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baf681-0d7f-4b3e-977c-eec58277765b",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1892e18-e2f2-4aad-afcb-14b6f4338f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    # Set the model to training mode (enables dropout, batch normalization, etc.)\n",
    "    model.train()\n",
    "\n",
    "    # Variable to accumulate losses for computing average loss later\n",
    "    losses = 0\n",
    "\n",
    "    # Load the training dataset\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    # Loop over each batch in the dataset\n",
    "    for src, tgt in train_dataloader:\n",
    "        # Move source and target tensors to the appropriate device (GPU or CPU)\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # Prepare the input and output for the target sequence\n",
    "        # The target input excludes the last token, and the output excludes the first token\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_out = tgt[1:, :]\n",
    "\n",
    "        # Create masks for the source and target sequences\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # Forward pass: compute predictions\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        # Zero the gradients before backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the loss between the predictions and the true target output\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        losses += loss.item()\n",
    "    \n",
    "    # Update the learning rate if using a learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Return the average loss\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a3c97d-5db7-468e-a4ab-b4f17fc60023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (disables dropout, batch normalization, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Variable to accumulate losses for computing average loss later\n",
    "    losses = 0\n",
    "\n",
    "    # Load the validation dataset\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    # Loop over each batch in the dataset\n",
    "    for src, tgt in val_dataloader:\n",
    "        # Move source and target tensors to the appropriate device\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # Prepare the input and output for the target sequence\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        tgt_out = tgt[1:, :]\n",
    "\n",
    "        # Create masks for the source and target sequences\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        # Forward pass: compute predictions\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        # Compute the loss between the predictions and the true target output\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        # Accumulate the loss\n",
    "        losses += loss.item()\n",
    "\n",
    "    # Return the average loss\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "add0eaf2-0cd4-4517-af40-f69a51bcd84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.637, Val loss: 5.068, Epoch time = 73.065s\n",
      "Epoch: 2, Train loss: 4.427, Val loss: 4.279, Epoch time = 72.763s\n",
      "Epoch: 3, Train loss: 3.847, Val loss: 3.974, Epoch time = 72.677s\n",
      "Epoch: 4, Train loss: 3.528, Val loss: 3.869, Epoch time = 72.738s\n",
      "Epoch: 5, Train loss: 3.325, Val loss: 3.724, Epoch time = 72.916s\n",
      "Epoch: 6, Train loss: 3.146, Val loss: 3.636, Epoch time = 73.246s\n",
      "Epoch: 7, Train loss: 3.007, Val loss: 3.614, Epoch time = 73.627s\n",
      "Epoch: 8, Train loss: 2.884, Val loss: 3.531, Epoch time = 73.844s\n",
      "Epoch: 9, Train loss: 2.753, Val loss: 3.466, Epoch time = 73.328s\n",
      "Epoch: 10, Train loss: 2.632, Val loss: 3.459, Epoch time = 73.405s\n",
      "Epoch: 11, Train loss: 2.537, Val loss: 3.468, Epoch time = 73.592s\n",
      "Epoch: 12, Train loss: 2.419, Val loss: 3.392, Epoch time = 73.231s\n",
      "Epoch: 13, Train loss: 2.304, Val loss: 3.328, Epoch time = 74.232s\n",
      "Epoch: 14, Train loss: 2.204, Val loss: 3.312, Epoch time = 73.543s\n",
      "Epoch: 15, Train loss: 2.105, Val loss: 3.320, Epoch time = 73.151s\n",
      "Epoch: 16, Train loss: 1.991, Val loss: 3.333, Epoch time = 73.269s\n",
      "Epoch: 17, Train loss: 1.891, Val loss: 3.313, Epoch time = 73.090s\n",
      "Epoch: 18, Train loss: 1.802, Val loss: 3.273, Epoch time = 73.098s\n",
      "Epoch: 19, Train loss: 1.699, Val loss: 3.251, Epoch time = 74.058s\n",
      "Epoch: 20, Train loss: 1.595, Val loss: 3.261, Epoch time = 74.433s\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdccd52-f9b8-4e5b-bfb4-00cd731e6af0",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch: 1, Train loss: 5.637, Val loss: 5.068, Epoch time = 73.065s\r\n",
    "Epoch: 2, Train loss: 4.427, Val loss: 4.279, Epoch time = 72.763s\r\n",
    "Epoch: 3, Train loss: 3.847, Val loss: 3.974, Epoch time = 72.677s\r\n",
    "Epoch: 4, Train loss: 3.528, Val loss: 3.869, Epoch time = 72.738s\r\n",
    "Epoch: 5, Train loss: 3.325, Val loss: 3.724, Epoch time = 72.916s\r\n",
    "Epoch: 6, Train loss: 3.146, Val loss: 3.636, Epoch time = 73.246s\r\n",
    "Epoch: 7, Train loss: 3.007, Val loss: 3.614, Epoch time = 73.627s\r\n",
    "Epoch: 8, Train loss: 2.884, Val loss: 3.531, Epoch time = 73.844s\r\n",
    "Epoch: 9, Train loss: 2.753, Val loss: 3.466, Epoch time = 73.328s\r\n",
    "Epoch: 10, Train loss: 2.632, Val loss: 3.459, Epoch time = 73.405s\r\n",
    "Epoch: 11, Train loss: 2.537, Val loss: 3.468, Epoch time = 73.592s\r\n",
    "Epoch: 12, Train loss: 2.419, Val loss: 3.392, Epoch time = 73.231s\r\n",
    "Epoch: 13, Train loss: 2.304, Val loss: 3.328, Epoch time = 74.232s\r\n",
    "Epoch: 14, Train loss: 2.204, Val loss: 3.312, Epoch time = 73.543s\r\n",
    "Epoch: 15, Train loss: 2.105, Val loss: 3.320, Epoch time = 73.151s\r\n",
    "Epoch: 16, Train loss: 1.991, Val loss: 3.333, Epoch time = 73.269s\r\n",
    "Epoch: 17, Train loss: 1.891, Val loss: 3.313, Epoch time = 73.090s\r\n",
    "Epoch: 18, Train loss: 1.802, Val loss: 3.273, Epoch time = 73.098s\r\n",
    "Epoch: 19, Train loss: 1.699, Val loss: 3.251, Epoch time = 74.058s\r\n",
    "Epoch: 20, Train loss: 1.595, Val loss: 3.261, Eoch time = 74.433s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68769d9e-316f-4958-b7e1-adaddedaaf15",
   "metadata": {},
   "source": [
    "## Testing it Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25871dde-7bac-454f-aebc-c4077109ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    Generate an output sequence using a greedy decoding algorithm.\n",
    "    \"\"\"\n",
    "    # Move source sequence and mask to the appropriate device\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    # Encode the source sequence\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # Initialize the target sequence with the start symbol\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "\n",
    "    # Generate the sequence token by token\n",
    "    for i in range(max_len-1):\n",
    "        # Move the memory tensor to the appropriate device (GPU or CPU)\n",
    "        memory = memory.to(device)\n",
    "    \n",
    "        # Generate a square subsequent mask for the target sequence so far.\n",
    "        # This mask ensures that the prediction for position i can depend only on the known outputs at positions less than i.\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(device)\n",
    "    \n",
    "        # Decode the output sequence so far to get the next output\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "    \n",
    "        # Transpose the output to make it suitable for generating probabilities from the generator\n",
    "        out = out.transpose(0, 1)\n",
    "    \n",
    "        # Pass the last output token through the generator to get logit probabilities for the next word\n",
    "        prob = model.generator(out[:, -1])\n",
    "    \n",
    "        # Select the token with the highest probability as the next word\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()  # Convert to a Python int\n",
    "    \n",
    "        # Append the predicted next word to the sequence (ys)\n",
    "        # The sequence (ys) holds all the tokens predicted so far and is used in the next iteration to predict the following token\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "    \n",
    "        # Break the loop if the end-of-sequence token is predicted\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    \n",
    "    return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c5674eb-f1de-4f7b-973d-493ee8a3a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    \"\"\"\n",
    "    Translate an input sentence into the target language using the model.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Transform the source sentence into a tensor\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "\n",
    "    # Create a source mask with all zeros\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "\n",
    "    # Generate the target sequence using the greedy_decode function\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "\n",
    "    # Convert the generated sequence of indices back to a string\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4166a24f-dfd8-4344-a2a3-45bc58f65c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ein Bauarbeiter wartet auf den nächsten Zug . \n"
     ]
    }
   ],
   "source": [
    "print(translate(model, \"It is cold outside .\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647a88e-768f-4f36-8e86-496c61a6109f",
   "metadata": {},
   "source": [
    "Prediction: `Ein Bauarbeiter wartet auf den nächsten Zug .`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa726f-42e2-4f60-ad53-6a2487986e92",
   "metadata": {},
   "source": [
    "Actual translation: `A construction worker waits for the next train`\n",
    "\n",
    "**Translation is off, but at least words form coherent sentences. We would probably need to train a lot more for actual translations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b4a9e-b34b-4c70-bb40-ab70bcbff7c7",
   "metadata": {},
   "source": [
    "## Binarization of Transformer Model\n",
    "The function of casting floating-point values into binary Binarized Neural Machine Translation values is summarized as follows:\n",
    "\n",
    "$$\n",
    "\\text{clip}(x, x_{\\min}, x_{\\max}) := \\min(x_{\\max}, \\max(x_{\\min}, x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_b := \\left\\lfloor \\text{clip}\\left( x, -1 + \\varepsilon, 1 - \\varepsilon \\right) + 0.5 \\right\\rfloor \\times B\n",
    "$$\n",
    "\n",
    "where $x$ is the input tensor, $\\varepsilon$ is a small floating-point number that prevents overflow when taking the floor, and $B$ is the binarization bound.\n",
    "\n",
    "### Note 1\n",
    "In my implementation, I only binarize the linear layer. In the actual paper, it would seem like the authors made a very custom implementation of each layer, and binarized each layer, and operations. I found it to be quite an involved process, so this serves as a demonstration.\n",
    "\n",
    "### Note 2\n",
    "The architecture I implemented below vs. the previous one: I added multiple Layer Norm layers in places where the original Transformer paper authors also placed the Layer Norms.\n",
    "\n",
    "The binarized linear layer also has a scaling factor involved, as mentioned in the paper. In this case, given that the feed forward layer is 4096, square root of that results to 64, so that's the default scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bbc0d47-67cf-419e-8caa-1647f55c4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to binarize weights\n",
    "def binarize_weights(weights, epsilon=0.1, B=2):\n",
    "    with torch.no_grad():\n",
    "        clipped = weights.clamp(-1 + epsilon, 1 - epsilon)\n",
    "        return torch.floor((clipped + 1 - epsilon) + 0.5) * B - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec89a8de-54c8-4e0e-adc5-3016fafa7ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarizedLinear(nn.Linear):\n",
    "    def forward(self, input, scaling_factor=64):\n",
    "        # Clone the original weights to preserve them for the backward pass\n",
    "        original_weight = self.weight.data.clone()\n",
    "\n",
    "        # Binarize the weights for the forward pass\n",
    "        # This converts the weights to -1 or +1, based on the binarization logic\n",
    "        self.weight.data = binarize_weights(self.weight.data)\n",
    "\n",
    "        # Forward pass with binarized weights and binarized input\n",
    "        # Binarizing the input as well to maintain consistency with weights\n",
    "        output = super(BinarizedLinear, self).forward(binarize_weights(input))\n",
    "\n",
    "        # Restore the original weights for the backward pass\n",
    "        # This ensures that the actual weights are used for gradient calculation\n",
    "        self.weight.data = original_weight\n",
    "\n",
    "        # Divide the output by a scaling factor\n",
    "        # This is typically done to counter the effect of binarization,\n",
    "        # which can significantly change the magnitude of the output\n",
    "        return output / scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98858e29-50b7-45b4-847c-7de9b5088e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarizedSixLTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_encoder_layers: int,\n",
    "                num_decoder_layers: int,\n",
    "                emb_size: int,\n",
    "                nhead: int,\n",
    "                src_vocab_size: int,\n",
    "                tgt_vocab_size: int,\n",
    "                dim_feedforward: int = 1024,\n",
    "                dropout: float = 0.1):\n",
    "        super(BinarizedSixLTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = BinarizedLinear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.src_tok_norm = LayerNorm(emb_size)\n",
    "        self.tgt_tok_norm = LayerNorm(emb_size)\n",
    "        self.final_norm = LayerNorm(emb_size)\n",
    "    \n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        # Apply LayerNorm after positional encoding\n",
    "        src_emb = self.src_tok_norm(src_emb)\n",
    "        tgt_emb = self.tgt_tok_norm(tgt_emb)\n",
    "\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        # Apply LayerNorm before generator\n",
    "        outs = self.final_norm(outs)\n",
    "\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        src_emb = self.src_tok_norm(src_emb)\n",
    "        return self.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        tgt_emb = self.tgt_tok_norm(tgt_emb)\n",
    "        return self.transformer.decoder(tgt_emb, memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e861c68-23e8-4b04-8076-5eb981b942d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 1024\n",
    "NHEAD = 16\n",
    "FFN_HID_DIM = 4096\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "\n",
    "# Initialize the model\n",
    "binarized_model = BinarizedSixLTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "binarized_model = binarized_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29979e14-fe4b-4ac0-95cd-e5828e461a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(binarized_model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bba4c770-1890-49eb-9166-720f0030bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in binarized_model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63c15194-0e26-4124-ad45-4363cfc34fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 9.861, Val loss: 9.859, Epoch time = 30.313s\n",
      "Epoch: 2, Train loss: 9.319, Val loss: 7.010, Epoch time = 29.779s\n",
      "Epoch: 3, Train loss: 6.222, Val loss: 6.772, Epoch time = 29.691s\n",
      "Epoch: 4, Train loss: 5.861, Val loss: 6.492, Epoch time = 29.832s\n",
      "Epoch: 5, Train loss: 5.752, Val loss: 6.281, Epoch time = 29.754s\n",
      "Epoch: 6, Train loss: 5.700, Val loss: 6.202, Epoch time = 29.781s\n",
      "Epoch: 7, Train loss: 5.667, Val loss: 6.190, Epoch time = 29.793s\n",
      "Epoch: 8, Train loss: 5.635, Val loss: 6.108, Epoch time = 29.834s\n",
      "Epoch: 9, Train loss: 5.592, Val loss: 6.053, Epoch time = 29.856s\n",
      "Epoch: 10, Train loss: 5.558, Val loss: 6.015, Epoch time = 30.001s\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(binarized_model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(binarized_model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfac8f8-9e7c-479c-b1b0-b825470fb36e",
   "metadata": {},
   "source": [
    "We also notice how the epochs take 30s as opposed to an average of 70-80s in the previous training operations. This is no coincidence - PyTorch optimizations seem to be happening in the background as the forward-passes are now much simpler to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ce808a7-dbc5-48c4-9bfc-f41a618e4a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ein Mann in . \n"
     ]
    }
   ],
   "source": [
    "print(translate(binarized_model, \"It is cold outside .\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c7fa9e-d84c-4e7a-8742-5ff8cab87c3f",
   "metadata": {},
   "source": [
    "Actual translation: `A man in .`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822ab70-0f2d-4d6c-9f48-0e716e4f92d8",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- Went over the generation of text embeddings for a sequence to sequence transformer (on 32 bits), with the purpose of Machine Translation.\n",
    "- Created a Transformers architecture suitable for machine translation, amd trained it in a similar fashion to what was presented within the paper.\n",
    "- Generated predictions using the trained model.\n",
    "- Successfully binarized a linear layer. Made sure that the binarization only takes place during the forward pass.\n",
    "- Trained a transformer architecture which includes the binarized linear layer and layer norms.\n",
    "- Outputted predictions using the binarized model.\n",
    "\n",
    "## Room for improvement\n",
    "- Binarizing all layers mentioned in the paper - this means custom implementations of all layers, including activations.\n",
    "- Training on the WMT 2014 data, as opposed to the Multi30k (larger dataset, longer training, better predictions).\n",
    "- Performing knowledge distillation given a pre-trained model for Machine Translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
